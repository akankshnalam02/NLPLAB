{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce167ead",
   "metadata": {},
   "source": [
    "### Excersise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2e38946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampletext\n"
     ]
    }
   ],
   "source": [
    "# Sample code to remove noisy words from a text\n",
    "noise_list = [\"is\", \"a\", \"this\", \"...\"]\n",
    "def _remove_noise (input_text): \n",
    "    words = input_text.split()\n",
    "    noise_free_words = [word for word in words if word not in noise_list]\n",
    "    noise_free_text = \"\".join(noise_free_words) \n",
    "    return noise_free_text\n",
    "print(_remove_noise(\"this is a sample text\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dbf4e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove this  from analytics vidhya\n"
     ]
    }
   ],
   "source": [
    "# Sample code to remove a regex pattern\n",
    "import re\n",
    "def remove_regex(input_text, regex_pattern):\n",
    "    result = re.sub(regex_pattern, '', input_text)\n",
    "    #result=re.sub(r'[\\s]+',\" \",result)\n",
    "    return result\n",
    "regex_pattern = \"#[\\w]*\"\n",
    "print(remove_regex(\"remove this #hashtag from analytics vidhya\",\n",
    "regex_pattern))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b64c0a",
   "metadata": {},
   "source": [
    "### Excersise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f0ce69e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\akanksh_02\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\akanksh_02\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\akanksh_02\\anaconda3\\lib\\site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\akanksh_02\\anaconda3\\lib\\site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\akanksh_02\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\akanksh_02\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58e86927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['so', 'mani', 'squid', 'are', 'jump']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "tokenized = [\"So\", \"many\", \"squids\", \"are\", \"jumping\"]\n",
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(token) for token in tokenized]\n",
    "print(stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b91701e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data',\n",
       " 'scienc',\n",
       " 'use',\n",
       " 'scientif',\n",
       " 'method',\n",
       " 'algorithm',\n",
       " 'and',\n",
       " 'mani',\n",
       " 'type',\n",
       " 'of',\n",
       " 'process']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stemmer = PorterStemmer()\n",
    "# stem words in the list of tokenised words\n",
    "def stem_words(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stems = [stemmer.stem(word) for word in word_tokens]\n",
    "    return stems\n",
    "text = \"data science uses scientific methods algorithms and many types of processes\"\n",
    "stem_words(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a104b3d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\akanksh_02\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\akanksh_02\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\akanksh_02\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foot\n",
      "['n', 'v', 'n', 'v', 'v', 'n', 'n', 'n', 'n', 'a']\n",
      "['The', 'strip', 'bat', 'be', 'hang', 'on', 'their', 'foot', 'for', 'best']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Ensure you have the necessary NLTK resources\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def get_wordnet_pos(word): \n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\" \n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper() \n",
    "    tag_dict = {\"J\": wordnet.ADJ, \n",
    "                \"N\": wordnet.NOUN, \n",
    "                \"V\": wordnet.VERB, \n",
    "                \"R\": wordnet.ADV} \n",
    " \n",
    "    return tag_dict.get(tag, wordnet.NOUN) \n",
    " \n",
    "lemmatizer = WordNetLemmatizer() \n",
    "word = 'feet' \n",
    "print(lemmatizer.lemmatize(word, get_wordnet_pos(word))) \n",
    " \n",
    "sentence = \"The striped bats are hanging on their feet for best\" \n",
    "print([get_wordnet_pos(w) for w in nltk.word_tokenize(sentence)]) \n",
    " \n",
    "print([lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in \n",
    "nltk.word_tokenize(sentence)]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051e1ddb",
   "metadata": {},
   "source": [
    "### Excersise 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a90fd9d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Retweet this is a retweeted tweet by students'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lookup_dict = {'rt':'Retweet', 'dm':'direct message', \"awsm\" : \"awesome\", \"luv\" :\"love\"} \n",
    "def _lookup_words(input_text): \n",
    "    words = input_text.split()  \n",
    "    new_words = []  \n",
    "    for word in words: \n",
    "        if word.lower() in lookup_dict: \n",
    "            word = lookup_dict[word.lower()] \n",
    "        new_words.append(word) \n",
    "    new_text = \" \".join(new_words)  \n",
    "    return new_text \n",
    "_lookup_words(\"RT this is a retweeted tweet by students\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe5b5e",
   "metadata": {},
   "source": [
    "### Excersise 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43eea6f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('on', 'IN'), ('Analytics', 'NNP'), ('Vidhya', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize, pos_tag \n",
    "text = \"I am learning Natural Language Processing on Analytics Vidhya\" \n",
    "tokens = word_tokenize(text) \n",
    "print(pos_tag(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d57d2064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['n', 'v', 'n', 'v', 'v', 'n', 'n', 'n', 'n', 'a']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet \n",
    " \n",
    "def get_wordnet_pos(word): \n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\" \n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper() \n",
    "    tag_dict = {\"J\": wordnet.ADJ, \n",
    "                \"N\": wordnet.NOUN, \n",
    "                \"V\": wordnet.VERB, \n",
    "                \"R\": wordnet.ADV} \n",
    " \n",
    "    return tag_dict.get(tag, wordnet.NOUN) \n",
    "sentence = \"The striped bats are hanging on their feet for best\" \n",
    "print([get_wordnet_pos(w) for w in nltk.word_tokenize(sentence)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f07a14",
   "metadata": {},
   "source": [
    "### Excersise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc93a0e",
   "metadata": {},
   "source": [
    "### Step1- Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2147f115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\akanksh_02\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\akanksh_02\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\akanksh_02\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for text preprocessing\n",
    "import re\n",
    "#import spacy\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "# import vectorizers\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# import numpy for matrix operation\n",
    "import numpy as np\n",
    "# import LDA from sklearn\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "# to suppress warnings\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "\n",
    "D1 = 'I want to watch a movie this weekend.'\n",
    "D2 = 'I went shopping yesterday. New Zealand won the World Test Championship by beating India by eight wickets at Southampton.'\n",
    "D3 = 'I don’t watch cricket. Netflix and Amazon Prime have very good movies to watch.'\n",
    "D4 = 'Movies are a nice way to chill however, this time I would like to paint and read some good books. Its been long!'\n",
    "D5 = 'This blueberry milkshake is so good! Try reading Dr. Joe Dispenzas books. His work is such a game-changer! His books helped to learn so much about how our thoughts impact our biology and how we can all rewire our brains.'\n",
    "\n",
    "# combining all the documents into a list:\n",
    "corpus = [D1, D2, D3, D4, D5]\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfe38a7",
   "metadata": {},
   "source": [
    "### Step2 Text Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a7c5484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "want watch movie weekend.\n",
      "want watch movie weekend\n",
      "went shopping yesterday. new zealand world test championship beating india eight wickets southampton.\n",
      "went shopping yesterday new zealand world test championship beating india eight wickets southampton\n",
      "don’t watch cricket. netflix amazon prime good movies watch.\n",
      "don’t watch cricket netflix amazon prime good movies watch\n",
      "movies nice way chill however, time would like paint read good books. long!\n",
      "movies nice way chill however time would like paint read good books long\n",
      "blueberry milkshake good! try reading dr. joe dispenzas books. work game-changer! books helped learn much thoughts impact biology rewire brains.\n",
      "blueberry milkshake good try reading dr joe dispenzas books work gamechanger books helped learn much thoughts impact biology rewire brains\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    print(stop_free)  # This is inside the function\n",
    "\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    print(punc_free)  # This is inside the function\n",
    "\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "clean_corpus = [clean(doc).split() for doc in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcceb85",
   "metadata": {},
   "source": [
    "###  step3 Convert Text into Numerical Representation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6a9856fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     amazon  beating   biology  blueberry      book     brain  championship  \\\n",
      "0  0.000000  0.00000  0.000000   0.000000  0.000000  0.000000       0.00000   \n",
      "1  0.000000  0.27735  0.000000   0.000000  0.000000  0.000000       0.27735   \n",
      "2  0.342983  0.00000  0.000000   0.000000  0.000000  0.000000       0.00000   \n",
      "3  0.000000  0.00000  0.000000   0.000000  0.237416  0.000000       0.00000   \n",
      "4  0.000000  0.00000  0.223316   0.223316  0.360339  0.223316       0.00000   \n",
      "\n",
      "      chill   cricket  dispenzas  ...     watch       way   weekend     went  \\\n",
      "0  0.000000  0.000000   0.000000  ...  0.458270  0.000000  0.568014  0.00000   \n",
      "1  0.000000  0.000000   0.000000  ...  0.000000  0.000000  0.000000  0.27735   \n",
      "2  0.000000  0.342983   0.000000  ...  0.553433  0.000000  0.000000  0.00000   \n",
      "3  0.294271  0.000000   0.000000  ...  0.000000  0.294271  0.000000  0.00000   \n",
      "4  0.000000  0.000000   0.223316  ...  0.000000  0.000000  0.000000  0.00000   \n",
      "\n",
      "    wicket      work    world     would  yesterday  zealand  \n",
      "0  0.00000  0.000000  0.00000  0.000000    0.00000  0.00000  \n",
      "1  0.27735  0.000000  0.27735  0.000000    0.27735  0.27735  \n",
      "2  0.00000  0.000000  0.00000  0.000000    0.00000  0.00000  \n",
      "3  0.00000  0.000000  0.00000  0.294271    0.00000  0.00000  \n",
      "4  0.00000  0.223316  0.00000  0.000000    0.00000  0.00000  \n",
      "\n",
      "[5 rows x 51 columns]\n"
     ]
    }
   ],
   "source": [
    "tf_idf_vectorizer = TfidfVectorizer( tokenizer=lambda doc:doc,lowercase=False)\n",
    "#tf_idf_vectorizer = TfidfVectorizer( lowercase=False)\n",
    "# Converting text into numerical representation\n",
    "#cv_vectorizer = CountVectorizer(tokenizer=lambda doc: doc,lowercase=False)\n",
    "# Array from TF-IDF Vectorizer\n",
    "tf_idf_arr = tf_idf_vectorizer.fit_transform(clean_corpus)\n",
    "# Array from Count Vectorizer\n",
    "#cv_arr = cv_vectorizer.fit_transform(clean_corpus)\n",
    "import pandas as pd\n",
    "df=pd.DataFrame(tf_idf_arr.toarray(),columns=tf_idf_vectorizer.get_feature_names_out())\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af64e577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.71953211 0.05629854 0.05603104 0.05607016 0.05603407 0.05603407]\n",
      " [0.03620009 0.03619922 0.81899585 0.03619839 0.03620323 0.03620323]\n",
      " [0.0449248  0.7761292  0.04472847 0.0447796  0.04471896 0.04471896]\n",
      " [0.03652797 0.03650944 0.03649304 0.81757645 0.03644655 0.03644655]\n",
      " [0.03142363 0.03146103 0.84279078 0.03147025 0.03142716 0.03142716]]\n"
     ]
    }
   ],
   "source": [
    "# Creating vocabulary array which will represent all the corpus\n",
    "vocab_tf_idf = tf_idf_vectorizer.get_feature_names_out()\n",
    "# get the vocb list\n",
    "vocab_tf_idf\n",
    "# Implementation of LDA:\n",
    "lda_model = LatentDirichletAllocation(n_components = 6, max_iter =20, random_state = 20)\n",
    "X_topics = lda_model.fit_transform(tf_idf_arr)\n",
    "print(X_topics)\n",
    "# .components_ gives us our topic distribution\n",
    "topic_words = lda_model.components_\n",
    "#print(topic_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f6e1b3",
   "metadata": {},
   "source": [
    "### step 4a Retrive the topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d5942d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### or for i, topic in enumerate(clean_corpus, start=1): print(f\"Topic {i} {topic}\") ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a17692c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amazon' 'beating' 'biology' 'blueberry' 'book' 'brain' 'championship'\n",
      " 'chill' 'cricket' 'dispenzas' 'don’t' 'dr' 'eight' 'gamechanger' 'good'\n",
      " 'helped' 'however' 'impact' 'india' 'joe' 'learn' 'like' 'long'\n",
      " 'milkshake' 'movie' 'much' 'netflix' 'new' 'nice' 'paint' 'prime' 'read'\n",
      " 'reading' 'rewire' 'shopping' 'southampton' 'test' 'thought' 'time' 'try'\n",
      " 'want' 'watch' 'way' 'weekend' 'went' 'wicket' 'work' 'world' 'would'\n",
      " 'yesterday' 'zealand']\n",
      "['like' 'would' 'way' 'however' 'time' 'nice' 'chill' 'paint' 'long'\n",
      " 'read' 'netflix' 'prime' 'amazon' 'cricket' 'don’t' 'rewire' 'thought'\n",
      " 'try' 'work' 'milkshake' 'reading' 'much' 'joe' 'biology' 'impact'\n",
      " 'helped' 'gamechanger' 'blueberry' 'dr' 'dispenzas' 'brain' 'learn'\n",
      " 'beating' 'world' 'wicket' 'went' 'championship' 'test' 'shopping'\n",
      " 'eight' 'new' 'yesterday' 'india' 'southampton' 'zealand' 'book' 'good'\n",
      " 'movie' 'watch' 'want' 'weekend']\n",
      "\n",
      " ------------------------------\n",
      "Topic 1 ['weekend' 'want' 'watch' 'movie' 'good']\n",
      "\n",
      " ------------------------------\n",
      "['amazon' 'beating' 'biology' 'blueberry' 'book' 'brain' 'championship'\n",
      " 'chill' 'cricket' 'dispenzas' 'don’t' 'dr' 'eight' 'gamechanger' 'good'\n",
      " 'helped' 'however' 'impact' 'india' 'joe' 'learn' 'like' 'long'\n",
      " 'milkshake' 'movie' 'much' 'netflix' 'new' 'nice' 'paint' 'prime' 'read'\n",
      " 'reading' 'rewire' 'shopping' 'southampton' 'test' 'thought' 'time' 'try'\n",
      " 'want' 'watch' 'way' 'weekend' 'went' 'wicket' 'work' 'world' 'would'\n",
      " 'yesterday' 'zealand']\n",
      "['however' 'would' 'read' 'paint' 'nice' 'way' 'time' 'chill' 'like'\n",
      " 'long' 'weekend' 'want' 'joe' 'learn' 'thought' 'milkshake' 'try' 'much'\n",
      " 'reading' 'helped' 'gamechanger' 'dr' 'dispenzas' 'work' 'brain'\n",
      " 'blueberry' 'biology' 'impact' 'rewire' 'went' 'wicket' 'test' 'world'\n",
      " 'southampton' 'zealand' 'new' 'yesterday' 'india' 'eight' 'championship'\n",
      " 'beating' 'shopping' 'book' 'movie' 'good' 'prime' 'netflix' 'don’t'\n",
      " 'cricket' 'amazon' 'watch']\n",
      "\n",
      " ------------------------------\n",
      "Topic 2 ['watch' 'amazon' 'cricket' 'don’t' 'netflix']\n",
      "\n",
      " ------------------------------\n",
      "['amazon' 'beating' 'biology' 'blueberry' 'book' 'brain' 'championship'\n",
      " 'chill' 'cricket' 'dispenzas' 'don’t' 'dr' 'eight' 'gamechanger' 'good'\n",
      " 'helped' 'however' 'impact' 'india' 'joe' 'learn' 'like' 'long'\n",
      " 'milkshake' 'movie' 'much' 'netflix' 'new' 'nice' 'paint' 'prime' 'read'\n",
      " 'reading' 'rewire' 'shopping' 'southampton' 'test' 'thought' 'time' 'try'\n",
      " 'want' 'watch' 'way' 'weekend' 'went' 'wicket' 'work' 'world' 'would'\n",
      " 'yesterday' 'zealand']\n",
      "['read' 'would' 'long' 'way' 'paint' 'nice' 'chill' 'however' 'like'\n",
      " 'time' 'prime' 'netflix' 'amazon' 'don’t' 'cricket' 'want' 'weekend'\n",
      " 'watch' 'movie' 'good' 'rewire' 'try' 'work' 'reading' 'thought' 'much'\n",
      " 'milkshake' 'biology' 'blueberry' 'brain' 'dispenzas' 'gamechanger' 'dr'\n",
      " 'impact' 'learn' 'helped' 'joe' 'eight' 'yesterday' 'india'\n",
      " 'championship' 'southampton' 'shopping' 'went' 'wicket' 'new' 'world'\n",
      " 'beating' 'test' 'zealand' 'book']\n",
      "\n",
      " ------------------------------\n",
      "Topic 3 ['book' 'zealand' 'test' 'beating' 'world']\n",
      "\n",
      " ------------------------------\n",
      "['amazon' 'beating' 'biology' 'blueberry' 'book' 'brain' 'championship'\n",
      " 'chill' 'cricket' 'dispenzas' 'don’t' 'dr' 'eight' 'gamechanger' 'good'\n",
      " 'helped' 'however' 'impact' 'india' 'joe' 'learn' 'like' 'long'\n",
      " 'milkshake' 'movie' 'much' 'netflix' 'new' 'nice' 'paint' 'prime' 'read'\n",
      " 'reading' 'rewire' 'shopping' 'southampton' 'test' 'thought' 'time' 'try'\n",
      " 'want' 'watch' 'way' 'weekend' 'went' 'wicket' 'work' 'world' 'would'\n",
      " 'yesterday' 'zealand']\n",
      "['amazon' 'cricket' 'prime' 'don’t' 'netflix' 'weekend' 'want' 'work'\n",
      " 'try' 'thought' 'rewire' 'reading' 'milkshake' 'learn' 'joe' 'impact'\n",
      " 'much' 'helped' 'gamechanger' 'biology' 'blueberry' 'brain' 'dr'\n",
      " 'dispenzas' 'world' 'beating' 'wicket' 'went' 'test' 'southampton'\n",
      " 'shopping' 'championship' 'zealand' 'india' 'eight' 'yesterday' 'new'\n",
      " 'watch' 'movie' 'good' 'book' 'time' 'read' 'way' 'paint' 'long' 'like'\n",
      " 'however' 'would' 'nice' 'chill']\n",
      "\n",
      " ------------------------------\n",
      "Topic 4 ['chill' 'nice' 'would' 'however' 'like']\n",
      "\n",
      " ------------------------------\n",
      "['amazon' 'beating' 'biology' 'blueberry' 'book' 'brain' 'championship'\n",
      " 'chill' 'cricket' 'dispenzas' 'don’t' 'dr' 'eight' 'gamechanger' 'good'\n",
      " 'helped' 'however' 'impact' 'india' 'joe' 'learn' 'like' 'long'\n",
      " 'milkshake' 'movie' 'much' 'netflix' 'new' 'nice' 'paint' 'prime' 'read'\n",
      " 'reading' 'rewire' 'shopping' 'southampton' 'test' 'thought' 'time' 'try'\n",
      " 'want' 'watch' 'way' 'weekend' 'went' 'wicket' 'work' 'world' 'would'\n",
      " 'yesterday' 'zealand']\n",
      "['long' 'would' 'nice' 'way' 'paint' 'however' 'chill' 'time' 'read'\n",
      " 'like' 'netflix' 'prime' 'amazon' 'don’t' 'cricket' 'want' 'weekend'\n",
      " 'reading' 'try' 'work' 'milkshake' 'rewire' 'thought' 'much' 'dr' 'joe'\n",
      " 'biology' 'impact' 'helped' 'gamechanger' 'blueberry' 'brain' 'learn'\n",
      " 'dispenzas' 'beating' 'world' 'wicket' 'went' 'championship' 'zealand'\n",
      " 'southampton' 'shopping' 'eight' 'india' 'new' 'yesterday' 'test' 'watch'\n",
      " 'book' 'movie' 'good']\n",
      "\n",
      " ------------------------------\n",
      "Topic 5 ['good' 'movie' 'book' 'watch' 'test']\n",
      "\n",
      " ------------------------------\n",
      "['amazon' 'beating' 'biology' 'blueberry' 'book' 'brain' 'championship'\n",
      " 'chill' 'cricket' 'dispenzas' 'don’t' 'dr' 'eight' 'gamechanger' 'good'\n",
      " 'helped' 'however' 'impact' 'india' 'joe' 'learn' 'like' 'long'\n",
      " 'milkshake' 'movie' 'much' 'netflix' 'new' 'nice' 'paint' 'prime' 'read'\n",
      " 'reading' 'rewire' 'shopping' 'southampton' 'test' 'thought' 'time' 'try'\n",
      " 'want' 'watch' 'way' 'weekend' 'went' 'wicket' 'work' 'world' 'would'\n",
      " 'yesterday' 'zealand']\n",
      "['long' 'would' 'nice' 'way' 'paint' 'however' 'chill' 'time' 'read'\n",
      " 'like' 'netflix' 'prime' 'amazon' 'don’t' 'cricket' 'want' 'weekend'\n",
      " 'reading' 'try' 'work' 'milkshake' 'rewire' 'thought' 'much' 'dr' 'joe'\n",
      " 'biology' 'impact' 'helped' 'gamechanger' 'blueberry' 'brain' 'learn'\n",
      " 'dispenzas' 'beating' 'world' 'wicket' 'went' 'championship' 'zealand'\n",
      " 'southampton' 'shopping' 'eight' 'india' 'new' 'yesterday' 'test' 'watch'\n",
      " 'book' 'movie' 'good']\n",
      "\n",
      " ------------------------------\n",
      "Topic 6 ['good' 'movie' 'book' 'watch' 'test']\n",
      "\n",
      " ------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Define the number of words that we want to print in every topic\n",
    "n_top_words = 5\n",
    "\n",
    "for i, topic_dist in enumerate(topic_words):\n",
    "    # np.argsort sorts an array or a list or the matrix according to their values\n",
    "    sorted_topic_dist = np.argsort(topic_dist)\n",
    "    \n",
    "    # Next, to view the actual words present in those indexes, we can make use of the vocab created earlier\n",
    "    print(np.array(vocab_tf_idf))\n",
    "    print(np.array(vocab_tf_idf)[sorted_topic_dist])\n",
    "    print(\"\\n ------------------------------\")\n",
    "    \n",
    "    # Extract the words from the vocabulary using the sorted_topic_indexes\n",
    "    # Obtain topics + words\n",
    "    # This topic_words variable contains the topics as well as the respective words present in those topics\n",
    "    topic_words = np.array(vocab_tf_idf)[sorted_topic_dist]\n",
    "    topic_words = topic_words[:-(n_top_words + 1):-1]\n",
    "    print(f\"Topic {i + 1}\", topic_words)\n",
    "    print(\"\\n ------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8421ab",
   "metadata": {},
   "source": [
    "### Step 4b Annotating the topics the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32bd0388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.71953211 0.05629854 0.05603104 0.05607016 0.05603407 0.05603407]\n",
      " [0.03620009 0.03619922 0.81899585 0.03619839 0.03620323 0.03620323]\n",
      " [0.0449248  0.7761292  0.04472847 0.0447796  0.04471896 0.04471896]\n",
      " [0.03652797 0.03650944 0.03649304 0.81757645 0.03644655 0.03644655]\n",
      " [0.03142363 0.03146103 0.84279078 0.03147025 0.03142716 0.03142716]]\n",
      "Document 1 -- Topic: 0\n",
      "Document 2 -- Topic: 2\n",
      "Document 3 -- Topic: 1\n",
      "Document 4 -- Topic: 3\n",
      "Document 5 -- Topic: 2\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have already defined lda_model and tf_idf_arr\n",
    "\n",
    "# To view the topics assigned to the documents:\n",
    "doc_topic = lda_model.transform(tf_idf_arr)\n",
    "print(doc_topic)\n",
    "\n",
    "# Iterating over every value until the end value\n",
    "for n in range(doc_topic.shape[0]):\n",
    "    # argmax() gives the maximum index value\n",
    "    topic_doc = doc_topic[n].argmax()\n",
    "    # Document is n+1\n",
    "    print(\"Document\", n+1, \"-- Topic:\", topic_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff2fbe2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
